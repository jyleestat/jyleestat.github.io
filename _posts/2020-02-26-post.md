---
title: "[Paper Review] Auto-Encoding Variational Bayes (2014)"
date: 2020-02-26
categories: VAE
---

Reference는 다음과 같습니다.
- [VAE: Kingma, Diederik P., and Max Welling. “Auto-encoding variational bayes.” arXiv preprint arXiv:1312.6114 (2014).](https://arxiv.org/pdf/1312.6114.pdf)
- [[Variational Autoencoder] Auto-Encoding Variational Bayes | AISC Foundational](https://www.youtube.com/watch?v=Tc-XfiDPLf4)
- [Woongwon Lee's blog - 'VAE Tutorial 2'](https://dnddnjs.github.io/paper/2018/06/20/vae2/)

### 0. 용어 정리
본 내용으로 들어가기 전 함수 용어를 먼저 정리하겠습니다.  

![image](https://user-images.githubusercontent.com/50100121/75211088-46baa680-57c6-11ea-8683-e4dc8fa92889.png)

&nbsp;


### 1. Introduction
#### 문제점
확률 모델의 사후 분포가 다루기 힘든 경우가 많습니다. 여기서, '다루기 힘든' 이란 적분이나 미분 등 수학적인 계산이 복잡하거나 함수를 정의하기 어려운 경우를 의미합니다. 기존의 Variational Bayes(VB)을 통해 다루기 힘든 사후분포의 근사치를 계산할 수 있지만, 그 근사치마저도 다루기 힘든 경우가 많습니다.
#### 해결책   
해당 논문은 **Auto-Encoding VB (AEVB) algorithm**을 선보입니다. **AEVB**는 **Stochastic Gradient Variational Bayes(SGVB)** 사후 분포의 근사치(recognition model)를 최적화하는 방법입니다. **SGVB**는 **reparameterization**을 사용한 lower bound에 대한 미분가능한 비편향 추정량이다. 

Paper에서 AEVB와 VB와의 차이점에 대해서 아래와 같이 언급합니다.
> Note that in contrast with the approximate posterior in mean-field variational inference, it is not necessarily factorial and its parameters ϕ are not computed from some closed-form expectation. (p.3)

&nbsp;



### 2. Lower Bound Estimator (SGVB) 구하기
#### 2.1 variational Lower Bound 구하기
lower bound estimator를 구하기 위해서는 *variational lower bound*를 먼저 구해야 합니다. 
먼저, x에 대한 marginal likelihood를 구하면 아래와 같습니다.  

<center><img src="https://user-images.githubusercontent.com/50100121/75218301-29450700-57dd-11ea-9f4b-9795be625211.png" width="520" height="240"></center>  

&nbsp;


오른쪽 식의 첫번째 항은 KL divergence 항이기 때문에 항상 양수입니다. 따라서 오른쪽 식의 두번째 항이 *(variational) lower bound* 입니다. *(variational) lower bound*를 정리하면 아래와 같습니다. (equation no. 는 paper와 동일합니다.)

<center><img src="https://user-images.githubusercontent.com/50100121/75218472-bab47900-57dd-11ea-8d70-89656686cb18.png" width="700" height="250"></center>  


eq.(2)와 eq.(3)은 동일한 수식입니다. 다만 eq.(3)의 KL divergence 값이 계산 가능한 경우에 (Appendix B 참고) eq.(3)을 사용합니다.  

&nbsp;

#### 2.2 reparameterization trick
#### 필요성
eq.(2) 또는 eq.(3)의 gradient를 구해서 stochastic하고 parameter를 업데이트하는 방식으로 training을 할 것입니다. θ에 대한 미분은 문제가 없으나, ϕ에 대한 미분에 문제가 있습니다. 미분값을 monte-carlo estimation을 통해 추정하게 된다면, gradient 값은 높은 variance를 갖게 됩니다. 이는 비효율적입니다. 따라서 **reparameterization**을 해결책으로 제안합니다.

#### 방법
<img src="https://user-images.githubusercontent.com/50100121/75219602-8db59580-57e0-11ea-9092-34fed87f7fab.png" width="900" height="220">

추가적인 변수 ϵ는 noise variable입니다. 즉 z를 posterior로부터 sampling 하는 것이 아니라 differentiable 한 함수 g로부터 deterministic하게 정해진다고 보는 것입니다. 그러면 recognition model에 대한 expectation이 ϵ에 대한 expectation으로 바꿔 표현할 수 있습니다. 이를 monte carlo estimation을 적용하면 마지막 수식으로 표현됩니다.  

Eq.(3)에 reparameterization trick을 적용하면 결과는 아래와 같습니다. 이 식을 **SGVB(Stochastic Gradient Variational Bayes)** 라고 합니다.

<center><img src="https://user-images.githubusercontent.com/50100121/75220464-d53d2100-57e2-11ea-9f42-58d69b2670d8.png" width="450" height="110"></center>

&nbsp;

### 3. AEVB Algorithm
위 과정을 거쳐 업데이트 알고리즘은 아래와 같습니다. (pseudo-code)
<center><img src="https://user-images.githubusercontent.com/50100121/75220702-601e1b80-57e3-11ea-9301-a20af4455b2a.png" width="650" height="230"></center>  
